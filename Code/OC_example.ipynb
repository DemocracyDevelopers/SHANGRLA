{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assertion RLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From 20 October 2022, major re-write and restructuring of the code and classes.**\n",
    "\n",
    "**From 18 May 2022, consistent sampling seems to be working.**\n",
    "\n",
    "**From 1 April 2022, integrating alpha_mart, p_history, and other features.**\n",
    "\n",
    "**From 24 October 2021, dev version implementing consistent sampling to target smaller contests.**\n",
    "\n",
    "## Overview of the assertion audit tool\n",
    "\n",
    "The tool requires as input:\n",
    "\n",
    "+ audit-specific and contest-specific parameters, such as\n",
    "    - whether to sample with or without replacement\n",
    "    - the name of the risk function to use, and any parameters it requires\n",
    "    - a risk limit for each contest to be audited\n",
    "    - the social choice function for each contest, including the number of winners\n",
    "    - candidate identifiers for each contest\n",
    "    - reported winner(s) for each contest\n",
    "    - an upper bound on the number of ballot cards that contain each contest\n",
    "    - an upper bound on the total number of cards across all contests\n",
    "    - whether to use card style information to target sampling\n",
    "+ a ballot manifest (see below)\n",
    "+ a random seed\n",
    "+ a file of cast vote records (for ballot-comparison audits)\n",
    "+ reported vote tallies for for each contest (for ballot-polling audits of plurality, supermajority, and approval social choice functions)\n",
    "+ json files of assertions for IRV contests (one file per IRV contest)\n",
    "+ human reading of voter intent from the paper cards selected for audit\n",
    "\n",
    "`use_style` controls whether the sample is drawn from all cards (`use_style == False`) or card style information is used\n",
    "to target the cards that purport to contain each contest (`use_style == True`).\n",
    "In the current implementation, card style information is inferred from cast-vote records, with additional 'phantom' CVRs if there could be more cards that contain a contest than is accounted for in the CVRs.\n",
    "Errors in the card style information are treated conservatively using the  \"phantoms-to-evil-zombies\" (~2EZ) approach ([Banuelos & Stark, 2012](https://arxiv.org/abs/1207.3413)) so that the risk limit remains valid, even if the CVRs misrepresent\n",
    "which cards contain which contests.\n",
    "\n",
    "The two ways of sampling are treated differently. \n",
    "If the sample is to be drawn only from cards that--according to the CVR--contain a particular contest, and a sampled card turns out not to\n",
    "contain that contest, that is considered a discrepancy, dealt with using the ~2EZ approach.\n",
    "It is assumed that every CVR corresponds to a card in the manifest, but there might\n",
    "be cards cast in the contest for which there is no corresponding CVR. In that case,\n",
    "phantom CVRs are created to ensure that the audit is still truly risk-limiting.\n",
    "\n",
    "Given an independent (i.e., not relying on the voting system) upper bound on the number of cards that contain the contest, if the number of CVRs that contain the contest does not exceed that bound, we can sample from paper purported to contain the contest and use the ~2EZ approach to deal with missing CVRs. This can greatly increase the efficiency of the audit if \n",
    "some contests appear on only a small percentage of the cast cards ([Glazer, Spertus, and Stark, 2021](https://dl.acm.org/doi/10.1145/3457907)).\n",
    "If there are more CVRs than the upper bound on the number of cards, extra CVRs can be deleted provided\n",
    "that deletion does not change any contest outcome. See [Stark, 2022](https://arxiv.org/abs/2207.01362).\n",
    "(However, if there more CVRs than cards, that is evidence of a process failure.)\n",
    "\n",
    "Any sampled phantom card (i.e., a card for which there is no CVR) is treated as if its CVR is a non-vote (which it is), and as if its MVR was least favorable (an \"evil zombie\" producing the greatest doubt in every assertion, separately). Any sampled card for which there is a CVR is compared to its corresponding CVR. \n",
    "If the card turns out not to contain the contest (despite the fact that the CVR says it does), the MVR is treated in the least favorable way for each assertion (i.e., as a zombie rather than as a non-vote).\n",
    "\n",
    "The tool helps select cards for audit, and reports when the audit has found sufficiently strong evidence to stop.\n",
    "\n",
    "The tool exports a log of all the audit inputs except the CVR file, but including the auditors' manually determined voter intent from the audited cards.\n",
    "\n",
    "The pre-10/2021 version used a single sample to audit all contests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal workflow\n",
    "\n",
    "+ Read overall audit information (including the seed) and contest information\n",
    "+ Read assertions for IRV contests and construct assertions for all other contests\n",
    "+ Read ballot manifest\n",
    "+ Read cvrs. Every CVR should have a corresponding manifest entry.\n",
    "+ Prepare ~2EZ:\n",
    "    - `N_phantoms = max_cards - cards_in_manifest`\n",
    "    - If `N_phantoms < 0`, complain\n",
    "    - Else create `N_phantoms` phantom cards\n",
    "    - For each contest `c`:\n",
    "        + `N_c` is the input upper bound on the number of cards that contain `c`\n",
    "        + if `N_c is None`, `N_c = max_cards - non_c_cvrs`, where `non_c_cvrs` is #CVRs that don't contain `c`\n",
    "        + `C_c` is the number of CVRs that contain the contest\n",
    "        + if `C_c > N_c`, complain\n",
    "        + else if `N_c - C_c > N_phantoms`, complain\n",
    "        + else:\n",
    "            - Consider contest `c` to be on the first `N_c - C_c` phantom CVRs\n",
    "            - Consider contest `c` to be on the first `N_c - C_c` phantom ballots\n",
    "+ Create Assertions for every Contest. This involves also creating an Assorter for every Assertion, and a `NonnegMean` test\n",
    "for every Assertion.\n",
    "+ Calculate assorter margins for all assorters:\n",
    "    - If `not use_style`, apply the Assorter to all cards and CVRs, including phantoms\n",
    "    - Else apply the assorter only to cards/cvrs reported to contain the contest, including phantoms that contain the contest\n",
    "+ Set `assertion.test.u` to the appropriate value for each assertion: `assorter.upper_bound` for polling audits or \n",
    "      `2/(2-assorter.margin/assorter.upper_bound)` for ballot-level comparison audits\n",
    "+ Estimate starting sample size for the specified sampling design (w/ or w/o replacement, stratified, etc.), for chosen risk function, use of card-style information, etc.:\n",
    "    - User-specified criterion, controlled by parameters. Examples:\n",
    "        + expected sample size for completion, on the assumption that there are no errors\n",
    "        + 90th percentile of sample size for completion, on the assumption that errors are not more frequent than specified\n",
    "    - If `not use_style`, base estimate on sampling from the entire manifest, i.e., smallest assorter margin\n",
    "    - Else use consistent sampling:\n",
    "        + Augment each CVR (including phantoms) with a probability of selection, `p`, initially 0\n",
    "        + For each contest `c`:\n",
    "            - Find sample size `n_c` that meets the criterion \n",
    "            - For each non-phantom CVR that contains the contest, set `p = max(p, n_c/N_c)` \n",
    "        + Estimated sample size is the sum of `p` over all non-phantom CVRs\n",
    "+ Draw the random sample:\n",
    "    - Use the specified design, including using consistent sampling for style information\n",
    "    - Express sample cards in terms of the manifest\n",
    "    - Export\n",
    "+ Read manual interpretations of the cards (MVRs)\n",
    "+ Calculate attained risk for each assorter\n",
    "    - Use ~2EZ to deal with phantom CVRs or cards; the treatment depends on whether `use_style == True`\n",
    "+ Report\n",
    "+ Estimate incremental sample size if any assorter nulls have not been rejected\n",
    "+ Draw incremental sample; etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit parameters.\n",
    "\n",
    "The overall audit involves information that is the same across contests, encapsulated in\n",
    "a dict called `audit`:\n",
    "\n",
    "* `seed`: the numeric seed for the pseudo-random number generator used to draw sample (for SHA256 PRNG)\n",
    "* `sim_seed`: seed for simulations to estimate sample sizes (for Mersenne Twister PRNG)\n",
    "* `quantile`: quantile of the sample size to use for setting initial sample size\n",
    "* `cvr_file`: filename for CVRs (input)\n",
    "* `manifest_file`: filename for ballot manifest (input)\n",
    "* `use_style`: Boolean. If True, use card style information (inferred from CVRs) to target samples. If False, sample from all cards, regardless of the contest.\n",
    "* `sample_file`: filename for sampled card identifiers (output)\n",
    "* `mvr_file`: filename for manually ascertained votes from sampled cards (input)\n",
    "* `log_file`: filename for audit log (output)\n",
    "* `error_rate_1`: expected rate of 1-vote overstatements. Recommended value $\\ge$ 0.001 if there are hand-marked ballots. Larger values increase the initial sample size, but make it more likely that the audit will conclude after a single round even if the audit finds errors\n",
    "* `error_rate_2`: expected rate of 2-vote overstatements. 2-vote overstatements should be extremely rare.\n",
    "Recommended value: 0. Larger values increase the initial sample size, but make it more likely that the audit will conclude after a single round even if the audit finds errors\n",
    "* `reps`: number of replications to use to estimate sample sizes. If `reps is None`, uses a deterministic method\n",
    "* `quantile`: quantile of sample size to estimate. Not used if `reps is None`\n",
    "* `strata`: a dict describing the strata. Keys are stratum identifiers; values are dicts containing:\n",
    "    + `max_cards`: an upper bound on the number of pieces of paper cast in the contest. This should be derived independently of the voting system. A ballot consists of one or more cards.\n",
    "    + `replacement`: whether to sample from this stratum with replacement. \n",
    "    + `use_style`: True if the sample in that stratum uses card-style information.\n",
    "    + `audit_type` one of Contest.POLLING, Contest.BALLOT_COMPARISON, Contest.BATCH_COMPARISON but only POLLING and BALLOT_COMPARISON are currently implemented. \n",
    "    + `test`: the name of the function to be used to measure risk. Options are `kaplan_markov`,`kaplan_wald`,`kaplan_kolmogorov`,`wald_sprt`,`kaplan_mart`, `alpha_mart`. \n",
    "Not all risk functions work with every social choice function or every sampling method. \n",
    "    + `estimator`: the estimator to be used by the risk function. Options are [FIX ME!]\n",
    "    + `test_kwargs`: keyword arguments for the risk function\n",
    "\n",
    "----\n",
    "\n",
    "* `contests`: a dict of contest-specific data \n",
    "    + the keys are unique contest identifiers for contests under audit\n",
    "    + the values are Contest objects with attributes:\n",
    "        - `risk_limit`: the risk limit for the audit of this contest\n",
    "        - `cards`: an upper bound on the number of cast cards that contain the contest\n",
    "        - `choice_function`: `Audit.SOCIAL_CHOICE_FUNCTION.PLURALITY`, \n",
    "          `Audit.SOCIAL_CHOICE_FUNCTION.SUPERMAJORITY`, or `Audit.SOCIAL_CHOICE_FUNCTION.IRV`\n",
    "        - `n_winners`: number of winners for majority contests. (Multi-winner IRV not supported)\n",
    "        - `share_to_win`: for super-majority contests, the fraction of valid votes required to win, e.g., 2/3. share_to_win*n_winners must be less than 100%)\n",
    "        - `candidates`: list of names or identifiers of candidates\n",
    "        - `reported_winners` : list of identifier(s) of candidate(s) reported to have won. Length should equal `n_winners`.\n",
    "        - `assertion_file`: filename for a set of json descriptors of Assertions (see technical documentation) that collectively imply the reported outcome of the contest is correct. Required for IRV; ignored for other social choice functions\n",
    "        - `audit_type`: the audit strategy. Currently `Audit.AUDIT_TYPE.POLLING (ballot-polling)` and \n",
    "           `Audit.AUDIT_TYPE.BALLOT_COMPARISON` (ballot-level comparison audits) are implemented. \n",
    "           HYBRID and STRATIFIED are planned.\n",
    "        - `test`: the risk function for the audit. Default is `NonnegMean.alpha_mart`, the alpha supermartingale test\n",
    "        - `estim`: estimator for the alternative hypothesis for the test. Default is `NonnegMean.shrink_trunc`\n",
    "        - `use_style`: True to use style information from CVRs to target the sample. False for polling audits or for sampling from all ballots for every contest.\n",
    "        - other keys and values are added by the software, including `cvrs`, the number of CVRs that contain the contest, and `p`, the sampling fraction expected to be required to confirm the contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from cryptorandom.cryptorandom import SHA256, int_from_hash\n",
    "from cryptorandom.sample import sample_by_index\n",
    "\n",
    "from Audit import Audit, Assertion, Assorter, Contest, CVR, Stratum\n",
    "from NonnegMean import NonnegMean\n",
    "from Dominion import Dominion\n",
    "from Hart import Hart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit = Audit.from_dict({\n",
    "         'seed':           12345678901234567890,\n",
    "         'sim_seed':       314159265,\n",
    "         'cvr_file':       '/Users/amanda/Downloads/oc_cvrs.zip', \n",
    "         #'cvr_file':       '/Users/Jake/Desktop/oc_cvrs.zip', \n",
    "         'manifest_file':  'Data/OC_mock_manifest_detailed.xlsx',\n",
    "         'sample_file':    '',\n",
    "         'mvr_file':       '',\n",
    "         'log_file':       'Data/OC_example_log.json',\n",
    "         'quantile':       0.8,\n",
    "         'error_rate_1':   0.001,\n",
    "         'error_rate_2':   0.0001,\n",
    "         'reps':           100,\n",
    "         'strata':         {'stratum_1': {'max_cards':   10010, \n",
    "                                          'use_style':   True,\n",
    "                                          'replacement': False,\n",
    "                                          'audit_type':  Audit.AUDIT_TYPE.BALLOT_COMPARISON,\n",
    "                                          'test':        NonnegMean.alpha_mart,\n",
    "                                          'estimator':   NonnegMean.optimal_comparison,\n",
    "                                          'test_kwargs': {}\n",
    "                                         }\n",
    "                           }\n",
    "        })\n",
    "\n",
    "# find upper bound on total cards across strata\n",
    "audit.max_cards = np.sum([s.max_cards for s in audit.strata.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvr_zip = \"/Users/Jake/Desktop/oc_cvrs.zip\"\n",
    "cvr_zip = \"/Users/amanda/Downloads/oc_cvrs.zip\"\n",
    "cvr_list = Hart.read_cvrs_zip(cvr_zip, size = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function Audit.CVR.tabulate_votes.<locals>.<lambda>()>,\n",
       "            {'Proposition 19': defaultdict(int,\n",
       "                         {'Yes': 1165, 'NA': 174, 'No': 1309}),\n",
       "             'Proposition 20': defaultdict(int,\n",
       "                         {'No': 2281, 'Yes': 1644, 'NA': 254}),\n",
       "             'Proposition 21': defaultdict(int,\n",
       "                         {'Yes': 2457, 'No': 4949, 'NA': 389}),\n",
       "             'Proposition 22': defaultdict(int,\n",
       "                         {'Yes': 5830, 'No': 3011, 'NA': 358}),\n",
       "             'Proposition 23': defaultdict(int,\n",
       "                         {'Yes': 2866, 'No': 5862, 'NA': 470}),\n",
       "             'Proposition 24': defaultdict(int,\n",
       "                         {'Yes': 5105, 'No': 4339, 'NA': 530}),\n",
       "             'Proposition 25': defaultdict(int,\n",
       "                         {'No': 5660, 'Yes': 3737, 'NA': 575}),\n",
       "             'AA-City of Orange': defaultdict(int,\n",
       "                         {'No': 283, 'Yes': 164, 'NA': 34}),\n",
       "             'Proposition 14': defaultdict(int,\n",
       "                         {'Yes': 13, 'No': 22, 'NA': 1}),\n",
       "             'Proposition 15': defaultdict(int,\n",
       "                         {'Yes': 17, 'No': 22, 'NA': 1}),\n",
       "             'Proposition 16': defaultdict(int,\n",
       "                         {'Yes': 28, 'No': 36, 'NA': 2}),\n",
       "             'Proposition 17': defaultdict(int,\n",
       "                         {'NA': 39, 'No': 544, 'Yes': 425}),\n",
       "             'Proposition 18': defaultdict(int,\n",
       "                         {'NA': 43, 'No': 726, 'Yes': 323}),\n",
       "             'Z-City of Newport Beach': defaultdict(int,\n",
       "                         {'No': 148, 'Yes': 170, 'NA': 50}),\n",
       "             'CC-City of Tustin': defaultdict(int,\n",
       "                         {'No': 42, 'Yes': 169, 'NA': 24}),\n",
       "             'S-City of Fullerton': defaultdict(int,\n",
       "                         {'Yes': 180, 'No': 221, 'NA': 17}),\n",
       "             'U-City of Fullerton': defaultdict(int,\n",
       "                         {'No': 236, 'Yes': 159, 'NA': 23}),\n",
       "             'DD-City of Westminster': defaultdict(int,\n",
       "                         {'Yes': 212, 'No': 70, 'NA': 19}),\n",
       "             'Y-City of Los Alamitos': defaultdict(int,\n",
       "                         {'No': 13, 'Yes': 24, 'NA': 1}),\n",
       "             'Q-City of Costa Mesa': defaultdict(int,\n",
       "                         {'Yes': 202, 'No': 114, 'NA': 24}),\n",
       "             'P-City of Cypress': defaultdict(int,\n",
       "                         {'Yes': 123, 'NA': 20, 'No': 48}),\n",
       "             'BB-City of San Clemente': defaultdict(int,\n",
       "                         {'Yes': 173, 'NA': 27, 'No': 42}),\n",
       "             'W-City of La Habra': defaultdict(int,\n",
       "                         {'Yes': 125, 'No': 53, 'NA': 9}),\n",
       "             'X-City of La Habra': defaultdict(int,\n",
       "                         {'No': 56, 'Yes': 122, 'NA': 9}),\n",
       "             'V-City of Laguna Woods': defaultdict(int,\n",
       "                         {'No': 46, 'Yes': 32, 'NA': 6}),\n",
       "             'ORANGE COUNTY WATER DISTRICT\\nDirector, Division 4': defaultdict(int,\n",
       "                         {'TRI TA': 4}),\n",
       "             'MUNICIPAL WATER DISTRICT OF ORANGE COUNTY\\nDirector, Division 3': defaultdict(int,\n",
       "                         {'TYLER DIEP': 4,\n",
       "                          'ROBERT R. \"BOB\" MCVICKER': 1,\n",
       "                          'LINDA MOULTON-PATTERSON': 1}),\n",
       "             'ORANGE COUNTY WATER DISTRICT\\nDirector, Division 6': defaultdict(int,\n",
       "                         {'CATHY GREEN': 1}),\n",
       "             'CITY OF GARDEN GROVE\\nMayor': defaultdict(int,\n",
       "                         {'STEVE JONES': 1}),\n",
       "             'IRVINE RANCH WATER DISTRICT\\nDirector, Division 4': defaultdict(int,\n",
       "                         {'MARY AILEEN MATHEIS': 1,\n",
       "                          'KAREN MCLAUGHLIN': 2,\n",
       "                          'NA': 1}),\n",
       "             'CITY OF WESTMINSTER\\nMember, City Council, District 3': defaultdict(int,\n",
       "                         {'KIMBERLY HO': 1}),\n",
       "             'MIDWAY CITY SANITARY DISTRICT\\nDirector': defaultdict(int,\n",
       "                         {'TYLER DIEP': 2, 'CHI CHARLIE NGUYEN': 1}),\n",
       "             'IRVINE UNIFIED SCHOOL DISTRICT\\nGoverning Board Member,\\nTrustee Area 3': defaultdict(int,\n",
       "                         {'BETHANY HUANG': 1}),\n",
       "             'CITY OF IRVINE\\nMayor': defaultdict(int,\n",
       "                         {'CHRISTINA L. SHEA': 1, 'FARRAH N. KHAN': 1}),\n",
       "             'CITY OF IRVINE\\nMember, City Council': defaultdict(int,\n",
       "                         {'JOHN PARK': 1,\n",
       "                          \"CARRIE O'MALLEY\": 1,\n",
       "                          'MIKE CARROLL': 1,\n",
       "                          'DYLAN GREEN': 1})})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CVR.tabulate_votes(cvr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Proposition 19': {'Yes': True},\n",
       " 'Proposition 20': {'No': True},\n",
       " 'Proposition 21': {'Yes': True},\n",
       " 'Proposition 22': {'Yes': True},\n",
       " 'Proposition 23': {'Yes': True},\n",
       " 'Proposition 24': {'Yes': True},\n",
       " 'Proposition 25': {'No': True},\n",
       " 'AA-City of Orange': {'No': True}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvr_list[0].votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contests to audit\n",
    "## Just choose Prop 19 for now to test\n",
    "contest_dict = {'Proposition 19':{\n",
    "                   'name': 'Prop 19',\n",
    "                   'risk_limit': 0.05,\n",
    "                   'cards': 1165 + 174 + 1309 + 10, # should create 10 phantoms\n",
    "                   'choice_function': Contest.SOCIAL_CHOICE_FUNCTION.PLURALITY, # in @Contest, not @Audit\n",
    "                   'n_winners': 1,\n",
    "                   'candidates': ['Yes','No'],\n",
    "                   'winner': ['No'],\n",
    "                   'assertion_file': None,\n",
    "                   'audit_type': Audit.AUDIT_TYPE.BALLOT_COMPARISON,\n",
    "                   'test': NonnegMean.alpha_mart,\n",
    "                   'estim': NonnegMean.optimal_comparison\n",
    "                  },\n",
    "                'Proposition 20':{\n",
    "                   'name': 'Prop 20',\n",
    "                   'risk_limit': 0.05,\n",
    "                   'cards': 2281 + 1644 + 254 + 10, # should create 10 phantoms\n",
    "                   'choice_function': Contest.SOCIAL_CHOICE_FUNCTION.PLURALITY, # in @Contest, not @Audit\n",
    "                   'n_winners': 1,\n",
    "                   'candidates': ['Yes','No'],\n",
    "                   'winner': ['No'],\n",
    "                   'assertion_file': None,\n",
    "                   'audit_type': Audit.AUDIT_TYPE.BALLOT_COMPARISON,\n",
    "                   'test': NonnegMean.alpha_mart,\n",
    "                   'estim': NonnegMean.optimal_comparison\n",
    "                  },\n",
    "                'Proposition 21':{\n",
    "                   'name': 'Prop 21',\n",
    "                   'risk_limit': 0.05,\n",
    "                   'cards': 2457 + 4949 + 389 + 10, # should create 10 phantoms\n",
    "                   'choice_function': Contest.SOCIAL_CHOICE_FUNCTION.PLURALITY, # in @Contest, not @Audit\n",
    "                   'n_winners': 1,\n",
    "                   'candidates': ['Yes','No'],\n",
    "                   'winner': ['No'],\n",
    "                   'assertion_file': None,\n",
    "                   'audit_type': Audit.AUDIT_TYPE.BALLOT_COMPARISON,\n",
    "                   'test': NonnegMean.alpha_mart,\n",
    "                   'estim': NonnegMean.optimal_comparison\n",
    "                  },\n",
    "                'V-City of Laguna Woods':{\n",
    "                   'name': 'Measure V',\n",
    "                   'risk_limit': 0.05,\n",
    "                   'cards': 46 + 32 + 6 + 10, # should create 10 phantoms\n",
    "                   'choice_function': Contest.SOCIAL_CHOICE_FUNCTION.PLURALITY, # in @Contest, not @Audit\n",
    "                   'n_winners': 1,\n",
    "                   'candidates': ['Yes','No'],\n",
    "                   'winner': ['No'],\n",
    "                   'assertion_file': None,\n",
    "                   'audit_type': Audit.AUDIT_TYPE.BALLOT_COMPARISON,\n",
    "                   'test': NonnegMean.alpha_mart,\n",
    "                   'estim': NonnegMean.optimal_comparison\n",
    "                  }\n",
    "               }\n",
    "\n",
    "contests = Contest.from_dict_of_dicts(contest_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of other social choice functions:\n",
    "\n",
    "        contests =  {'city_council':{'name': 'City Council',\n",
    "                             'risk_limit':0.05,\n",
    "                             'cards': None,\n",
    "                             'choice_function': Contest.SOCIAL_CHOICE_FUNCTION.PLURALITY,\n",
    "                             'n_winners':3,\n",
    "                             'candidates':['Doug','Emily','Frank','Gail','Harry'],\n",
    "                             'winner' : ['Doug', 'Emily', 'Frank']\n",
    "                            },\n",
    "                        'measure_1':{'name': 'Measure 1',\n",
    "                             'risk_limit':0.05,\n",
    "                             'cards': 65432,\n",
    "                             'choice_function': Contest.SOCIAL_CHOICE_FUNCTION.SUPERMAJORITY,\n",
    "                             'share_to_win':2/3,\n",
    "                             'n_winners':1,\n",
    "                             'candidates':['yes','no'],\n",
    "                             'winner' : ['yes']\n",
    "                            }                  \n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the assertions for the IRV contest\n",
    "for c in contests:\n",
    "    if contests[c].choice_function == Contest.SOCIAL_CHOICE_FUNCTION.IRV:\n",
    "        with open(contests[c].assertion_file, 'r') as f:\n",
    "            contests[c].assertion_json = json.load(f)['audits'][0]['assertions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct the dict of dicts of assertions for each contest\n",
    "Assertion.make_all_assertions(contests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audit.check_audit_parameters(contests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the ballot manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special for Primary/Dominion manifest format\n",
    "manifest = pd.read_excel(audit.manifest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch Name</th>\n",
       "      <th>Number of Ballots</th>\n",
       "      <th>Container</th>\n",
       "      <th>Tabulator</th>\n",
       "      <th>cum_cards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1706</td>\n",
       "      <td>1</td>\n",
       "      <td>99808</td>\n",
       "      <td>1706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>99808</td>\n",
       "      <td>1770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>99808</td>\n",
       "      <td>1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>99808</td>\n",
       "      <td>1856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>99808</td>\n",
       "      <td>1925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Batch Name  Number of Ballots  Container  Tabulator  cum_cards\n",
       "0           0               1706          1      99808       1706\n",
       "1          10                 64          1      99808       1770\n",
       "2         100                 80          1      99808       1850\n",
       "3         101                  6          1      99808       1856\n",
       "4         102                 69          1      99808       1925"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the CVR data and create CVR objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ballot-level comparison audits\n",
    "#cvr_list, cvrs_read, unique_ids = CVR.from_raire_file(audit.cvr_file)\n",
    "cvr_list = Hart.read_cvrs_zip(audit.cvr_file, size = 10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'274_2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvr_list[0].id # SHOULD SWITCH THIS TO - to be in line with dominion I believe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10010, 10000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether the manifest accounts for every card\n",
    "# it doesn't because phantoms \n",
    "audit.max_cards, np.sum(manifest['Number of Ballots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amanda/Dropbox (Personal)/RLAs/SHANGRLA/Code/Hart.py:60: UserWarning: manifest does not account for every card; appending batch of 10 phantom cards to the manifest\n",
      "  warnings.warn(f'manifest does not account for every card; appending batch of {phantoms} phantom cards to the manifest')\n",
      "/Users/amanda/Dropbox (Personal)/RLAs/SHANGRLA/Code/Hart.py:63: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  manifest = manifest.append(r, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "# Check that there is a card in the manifest for every card (possibly) cast. If not, add phantoms.\n",
    "manifest, manifest_cards, phantom_cards = Hart.prep_manifest(manifest, audit.max_cards, len(cvr_list))\n",
    "#manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10010, 10010)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Note: for some reason prep manifest turns all the columns into string type..\n",
    "audit.max_cards, np.sum(manifest['Number of Ballots'].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CVRs for phantom cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 13 phantom records\n"
     ]
    }
   ],
   "source": [
    "# For Comparison Audits Only\n",
    "#----------------------------\n",
    "\n",
    "# If the sample draws a phantom card, these CVRs will be used in the comparison.\n",
    "# phantom MVRs should be treated as zeros by the Assorter for every contest\n",
    "\n",
    "# setting use_style = False to generate phantoms\n",
    "\n",
    "cvr_list, phantom_vrs = CVR.make_phantoms(audit=audit, contests=contests, \n",
    "                                          cvr_list=cvr_list, prefix='phantom-1-')\n",
    "print(f\"Created {phantom_vrs} phantom records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum assorter margin 0.05417607223476306\n",
      "margins in contest Proposition 19:\n",
      "\tassertion No v Yes: 0.05417607223476306\n",
      "margins in contest Proposition 20:\n",
      "\tassertion No v Yes: 0.15206493196466941\n",
      "margins in contest Proposition 21:\n",
      "\tassertion No v Yes: 0.31928251121076223\n",
      "margins in contest V-City of Laguna Woods:\n",
      "\tassertion No v Yes: 0.14893617021276606\n"
     ]
    }
   ],
   "source": [
    "# find the mean of the assorters for the CVRs and check whether the assertions are met\n",
    "min_margin = Assertion.set_margins_from_cvrs(audit=audit, contests=contests, cvr_list=cvr_list)\n",
    "\n",
    "print(f'minimum assorter margin {min_margin}')\n",
    "Contest.print_margins(contests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit.write_audit_parameters(contests=contests) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find initial sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_size=199\n",
      "[('Proposition 19', 133), ('Proposition 20', 47), ('Proposition 21', 22), ('V-City of Laguna Woods', 40)]\n"
     ]
    }
   ],
   "source": [
    "# find initial sample size \n",
    "# error here, there are some CVRs that aren't receiving a p, \n",
    "# which then causes an error when they are summed over. \n",
    "# See line 746-749 in Audit.py;\n",
    "# one fix is to set the p for CVRs not in the contest to 0; \n",
    "# another is to skip counting them in the sum...\n",
    "sample_size = audit.find_sample_size(contests, cvrs=cvr_list)  \n",
    "print(f'{sample_size=}\\n{[(i, c.sample_size) for i, c in contests.items()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Draw the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample includes 5 phantom cards.\n"
     ]
    }
   ],
   "source": [
    "# draw the initial sample using consistent sampling\n",
    "prng = SHA256(audit.seed)\n",
    "CVR.assign_sample_nums(cvr_list, prng)\n",
    "#sampled_cvr_indices needs to be an array for Hart.sample_from_cvrs?\n",
    "#why are we subtracting 1 from it, i.e. `enumerate(sample-1)` in Hart.sample_from_cvrs\n",
    "sampled_cvr_indices = CVR.consistent_sampling(cvr_list=cvr_list, contests=contests)\n",
    "n_sampled_phantoms = np.sum(sampled_cvr_indices > manifest_cards)\n",
    "print(f'The sample includes {n_sampled_phantoms} phantom cards.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10013, 10000, 10010)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cvr_list), manifest_cards, audit.max_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison audit\n",
    "cards_to_retrieve, sample_order, cvr_sample, mvr_phantoms_sample = \\\n",
    "    Hart.sample_from_cvrs(cvr_list, manifest, sampled_cvr_indices)\n",
    "\n",
    "# for polling audit\n",
    "# cards_to_retrieve, sample_order, mvr_phantoms_sample = Dominion.sample_from_manifest(manifest, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phantom-1-3'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvr_sample[69].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the sample\n",
    "#Dominion.write_cards_sampled(audit.sample_file, cards_to_retrieve, print_phantoms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the audited sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for real data\n",
    "# with open(audit.mvr_file) as f:\n",
    "#     mvr_json = json.load(f)\n",
    "\n",
    "# mvr_sample = CVR.from_dict(mvr_json['ballots'])\n",
    "\n",
    "# for simulated data, no errors\n",
    "mvr_sample = cvr_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find measured risks for all assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "phantom-1-3\n"
     ]
    }
   ],
   "source": [
    "### ISSUE HERE because phantom has no selection order which flags error\n",
    "# in prep_comparison_sample in Audit.py\n",
    "j = 0\n",
    "for x in mvr_sample:\n",
    "    if x.id == \"phantom-1-3\":\n",
    "        print(j)\n",
    "    j = j + 1\n",
    "    \n",
    "print(mvr_sample[69].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selection_order': 69, 'serial': 10003}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_order[mvr_sample[69].id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvr_sample.sort(key = lambda x: sample_order[x.id][\"selection_order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SEE ABOVE FOR WHY THIS IS FLAGGING ERROR\n",
    "CVR.prep_comparison_sample(mvr_sample, cvr_sample, sample_order)  # for comparison audit\n",
    "# CVR.prep_polling_sample(mvr_sample, sample_order)  # for polling audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum assertion p-value 1.0\n",
      "\n",
      "p-values for assertions in contest Proposition 19\n",
      "\tNo v Yes: 0.24999608501283765\n",
      "\n",
      "contest Proposition 19 audit INCOMPLETE at risk limit 0.05. Attained risk 0.24999608501283765\n",
      "assertions remaining to be proved:\n",
      "\tcontest_id: Proposition 19 winner: No loser: Yes assorter: contest_id: Proposition 19\n",
      "upper bound: 1, winner defined: False, loser defined: False, assort defined: True p-value: 0.24999608501283765 margin: 0.05417607223476306 test: test: <bound method NonnegMean.alpha_mart of <NonnegMean.NonnegMean object at 0x7fea12120460>> estim: <bound method NonnegMean.optimal_comparison of <NonnegMean.NonnegMean object at 0x7fea12120460>> upper bound u: 1.0278422273781902 N: 2658 null mean t: 0.5 kwargs: {'g': 0.1} p-history length: 136 proved: False sample_size: 133 assorter upper bound: 1: current risk 0.24999608501283765\n",
      "\n",
      "p-values for assertions in contest Proposition 20\n",
      "\tNo v Yes: 1.5042537829185744e-05\n",
      "\n",
      "contest Proposition 20 AUDIT COMPLETE at risk limit 0.05. Attained risk 1.5042537829185744e-05\n",
      "\n",
      "p-values for assertions in contest Proposition 21\n",
      "\tNo v Yes: 1.1490508229431133e-13\n",
      "\n",
      "contest Proposition 21 AUDIT COMPLETE at risk limit 0.05. Attained risk 1.1490508229431133e-13\n",
      "\n",
      "p-values for assertions in contest V-City of Laguna Woods\n",
      "\tNo v Yes: 1\n",
      "\n",
      "contest V-City of Laguna Woods audit INCOMPLETE at risk limit 0.05. Attained risk 1\n",
      "assertions remaining to be proved:\n",
      "\tcontest_id: V-City of Laguna Woods winner: No loser: Yes assorter: contest_id: V-City of Laguna Woods\n",
      "upper bound: 1, winner defined: False, loser defined: False, assort defined: True p-value: 1 margin: 0.14893617021276606 test: test: <bound method NonnegMean.alpha_mart of <NonnegMean.NonnegMean object at 0x7fea121200d0>> estim: <bound method NonnegMean.optimal_comparison of <NonnegMean.NonnegMean object at 0x7fea121200d0>> upper bound u: 1.0804597701149425 N: 94 null mean t: 0.5 kwargs: {'g': 0.1} p-history length: 40 proved: False sample_size: 40 assorter upper bound: 1: current risk 1\n"
     ]
    }
   ],
   "source": [
    "p_max = Assertion.set_p_values(contests=contests, mvr_sample=mvr_sample, cvr_sample=cvr_sample)\n",
    "print(f'maximum assertion p-value {p_max}')\n",
    "done = audit.summarize_status(contests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the status of the audit \n",
    "audit.write_audit_parameters(contests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'Proposition 20', 'name': 'Prop 20', 'risk_limit': 0.05, 'cards': 4189, 'choice_function': 'PLURALITY', 'n_winners': 1, 'share_to_win': None, 'candidates': ['Yes', 'No'], 'winner': ['No'], 'assertion_file': None, 'audit_type': 'BALLOT_COMPARISON', 'test': <function NonnegMean.alpha_mart at 0x7fea31a2b0d0>, 'g': 0.1, 'estim': <function NonnegMean.optimal_comparison at 0x7fea31a2b4c0>, 'use_style': True, 'assertions': {'No v Yes': <Audit.Assertion object at 0x7fea121202b0>}, 'tally': None, 'sample_size': 47, 'cvrs': 4176, 'margins': {'No v Yes': 0.15206493196466941}, 'p_values': {'No v Yes': 1.5042537829185744e-05}, 'proved': {'No v Yes': 1}, 'max_p': 1.5042537829185744e-05}\n"
     ]
    }
   ],
   "source": [
    "print(contests['Proposition 20'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many more cards should be audited?\n",
    "\n",
    "Estimate how many more cards will need to be audited to confirm any remaining contests. The enlarged sample size is based on:\n",
    "\n",
    "* cards already sampled\n",
    "* the assumption that we will continue to see errors at the same rate observed in the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_size=363\n",
      "[('Proposition 19', 133), ('Proposition 20', 0), ('Proposition 21', 0), ('V-City of Laguna Woods', 40)]\n"
     ]
    }
   ],
   "source": [
    "sample_size = audit.find_sample_size(contests, cvr_list, mvr_sample)\n",
    "print(f'{sample_size=}\\n{[(i, c.sample_size) for i, c in contests.items()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment the sample\n",
    "# reset the seed\n",
    "prng = SHA256(seed)\n",
    "old_sample = sample\n",
    "sample = sample_by_index(max_cards, new_size, prng=prng)\n",
    "incremental_sample = np.sort(list(set(sample) - set(old_sample)))\n",
    "n_phantom_sample = np.sum([cvr_list[i].phantom for i in incremental_sample])\n",
    "print(\"The incremental sample includes {} phantom cards.\".format(n_phantom_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvr_sample_lookup_new, cvr_sample_new, mvr_phantoms_sample_new = \\\n",
    "                sample_from_cvrs(cvr_list, manifest, incremental_sample)\n",
    "write_cards_sampled(sample_file, cvr_sample_lookup_new, print_phantoms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mvr_json should contain the complete set of mvrs, including those in previous rounds\n",
    "\n",
    "with open(mvr_file) as f:\n",
    "    mvr_json = json.load(f)\n",
    "\n",
    "mvr_sample = CVR.from_dict(mvr_json['ballots']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile entire sample\n",
    "cvr_sample_lookup, cvr_sample, mvr_phantoms_sample = sample_from_cvrs(cvr_list, manifest, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add MVRs for phantoms\n",
    "mvr_sample = mvr_sample + mvr_phantoms_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find measured risks for all assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_sample(mvr_sample, cvr_sample)\n",
    "p_max = find_p_values(contests, mvr_sample, cvr_sample, manifest_type, \\\n",
    "                      risk_function= risk_fn)\n",
    "print(\"maximum assertion p-value {}\".format(p_max))\n",
    "done = summarize_status(contests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the status of the audit \n",
    "write_audit_parameters(log_file, seed, replacement, risk_function, g, max_cards, len(cvr_list), \\\n",
    "                       manifest_cards, phantom_cards, error_rate, contests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(5)\n",
    "y = x\n",
    "y[3]=2\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIT_TYPES = (IRV:=\"IRV\", other:=\"OTHER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIT_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import json\n",
    "import csv\n",
    "import warnings\n",
    "import typing\n",
    "from numpy import testing\n",
    "from collections import OrderedDict, defaultdict\n",
    "from cryptorandom.cryptorandom import SHA256, random, int_from_hash\n",
    "from cryptorandom.sample import random_permutation\n",
    "from cryptorandom.sample import sample_by_index\n",
    "\n",
    "from CVR import CVR\n",
    "from Audit import Audit, Assertion, Assorter, Contest\n",
    "from NonnegMean import NonnegMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.01\n",
    "N = int(10**4)\n",
    "margin = 0.1\n",
    "upper_bound = 1\n",
    "u = 2/(2-margin/upper_bound)\n",
    "m = (1 - rate*upper_bound/margin)/(2*upper_bound/margin - 1)\n",
    "one_over = 1/3.8 # 0.5/(2-margin)\n",
    "clean = 1/1.9    # 1/(2-margin)\n",
    "\n",
    "AvB = Contest.from_dict({'id': 'AvB',\n",
    "                     'name': 'AvB',\n",
    "                     'risk_limit': 0.05,\n",
    "                     'cards': 10**4,\n",
    "                     'choice_function': Audit.SOCIAL_CHOICE_FUNCTION.PLURALITY,\n",
    "                     'n_winners': 1,\n",
    "                     'candidates': ['Alice','Bob', 'Carol'],\n",
    "                     'winners': ['Alice'],\n",
    "                     'audit_type': Audit.AUDIT_TYPE.BALLOT_COMPARISON,\n",
    "                     'test': NonnegMean.kaplan_markov,\n",
    "                     'tally': {'Alice': 3000, 'Bob': 2000, 'Carol': 1000},\n",
    "                     'g': 0.1,\n",
    "                     'use_style': True\n",
    "                })\n",
    "losers = list(set(AvB.candidates)-set(AvB.winners))\n",
    "AvB.assertions = Assertion.make_plurality_assertions(AvB, winners=AvB.winners, losers=losers)\n",
    "AvB.find_margins_from_tally()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first test\n",
    "for a_id, a in AvB.assertions.items():\n",
    "    sam_size1 = a.find_sample_size(data=np.ones(10), prefix=True, rate=rate, reps=None, quantile=0.5, seed=1234567890)\n",
    "    # Kaplan-Markov martingale is \\prod (t+g)/(x+g). For x = [1, 1, ...], sample size should be:\n",
    "    ss1 = math.ceil(np.log(AvB.risk_limit)/np.log((a.test.t+a.test.g)/(1+a.test.g)))\n",
    "    print(f'{sam_size1} {ss1}')\n",
    "    assert sam_size1 == ss1\n",
    "    # For \"clean\", the term is (1/2+g)/(clean+g); for one_over\n",
    "    # it is (1/2+g)/(one_over+g). \n",
    "    clean = 1/(2-a.margin/a.assorter.upper_bound)\n",
    "    over = clean/2\n",
    "    c = (a.test.t+a.test.g)/(clean+a.test.g)\n",
    "    o = (a.test.t+a.test.g)/(clean/2+a.test.g)\n",
    "    sam_size2 = a.find_sample_size(data=None, prefix=True, rate=rate, reps=10**3, quantile=0.5, seed=1234567890)\n",
    "    ss2 = 1+math.ceil(np.log(AvB.risk_limit/o)/np.log(c))\n",
    "    assert sam_size2 == ss2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0.957983193277311; o=1.652173913043478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o*c**(sam_size2-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "state": {
    "6cab9cab294247839758fa9e8d64d122": {
     "views": [
      {
       "cell_index": 42
      }
     ]
    },
    "b7b0321f834d45ebb1bdc036fba7a916": {
     "views": [
      {
       "cell_index": 38
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
